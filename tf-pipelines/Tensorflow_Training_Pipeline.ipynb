{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpd34b2co2JgCFiCX8iwCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayid-alt/mymachine-learning/blob/main/tf-pipelines/Tensorflow_Training_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT LIBRARY**"
      ],
      "metadata": {
        "id": "bqmj-fEroYCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXTbr4-joGNc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_url = 'https://raw.githubusercontent.com/natashayulian/diamond_dataset/master/diamonds.csv'\n",
        "df = pd.read_csv(csv_url, index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zfudUhn9oMNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dtype = pd.DataFrame({\n",
        "    'dtype' : df.dtypes.apply(lambda x : 'Numeric' if x != 'object' else 'Categorical')\n",
        "}).reset_index()\n",
        "df_dtype"
      ],
      "metadata": {
        "id": "wekfcokvoQVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPLIT DATASET**"
      ],
      "metadata": {
        "id": "mgJ30zxftkMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "metadata": {
        "id": "f5asep22ow9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 = low; 1 = high\n",
        "df['target'] = np.where(df['price'] <= 1000, 0, 1)\n",
        "# Drop un-used columns.\n",
        "df = df.drop(columns=['price'])"
      ],
      "metadata": {
        "id": "to1H-ULnoww7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RifIXkRrpvLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataframe Transformation**"
      ],
      "metadata": {
        "id": "wiQhsxzMtjKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = df.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "\n",
        "  return ds\n",
        "\n",
        "batch_size=10\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zDvhkbVOp07M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FEATURE COLUMNS**"
      ],
      "metadata": {
        "id": "bce42pGPtuoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Numeric Column**"
      ],
      "metadata": {
        "id": "ZdOrshcwzqA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = next(iter(train_ds))[0]\n",
        "def demo(feature_columns):\n",
        "  feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "  print(feature_layer(example_batch).numpy())\n",
        "\n",
        "\n",
        "carat = feature_column.numeric_column('carat')\n",
        "demo(carat)"
      ],
      "metadata": {
        "id": "rsyFPIo1qkGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Bucketized Column**"
      ],
      "metadata": {
        "id": "KZspFc0lzszg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bucketized column\n",
        "carat = feature_column.numeric_column('carat')\n",
        "carat_buckets = feature_column.bucketized_column(carat, boundaries=[1, 2])\n",
        "demo(carat_buckets)"
      ],
      "metadata": {
        "id": "h6eev1FmqGGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Categorical column**"
      ],
      "metadata": {
        "id": "njELQAI0PiRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_type = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'color', ['E', 'I','J','D','H', 'G','F'])\n",
        "\n",
        "color_type_one_hot = feature_column.indicator_column(color_type)\n",
        "demo(color_type_one_hot)"
      ],
      "metadata": {
        "id": "RSKzsQvO4T51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Embedding Feature**"
      ],
      "metadata": {
        "id": "QzLj1bBpQRfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding\n",
        "clarity = feature_column.categorical_column_with_vocabulary_list(\n",
        "      'clarity', df.clarity.unique())\n",
        "clarity_embedding = feature_column.embedding_column(clarity, dimension=6)\n",
        "demo(clarity_embedding)"
      ],
      "metadata": {
        "id": "t-tkML9dPqdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Hashed feature column**"
      ],
      "metadata": {
        "id": "2-OKi3tjbeQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clarity_hashed = feature_column.categorical_column_with_hash_bucket(\n",
        "      'clarity', hash_bucket_size=5)\n",
        "demo(feature_column.indicator_column(clarity_hashed))\n"
      ],
      "metadata": {
        "id": "g-I4W64oQTlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Crossed feature column**"
      ],
      "metadata": {
        "id": "jc-fPhlmbqoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cross feature\n",
        "#data yang di cross harus berupa string, categorical, atau bucketized\n",
        "crossed_feature = feature_column.crossed_column([carat_buckets, color_type],\n",
        "                                                hash_bucket_size=10)\n",
        "demo(feature_column.indicator_column(crossed_feature))"
      ],
      "metadata": {
        "id": "xUsHErasbkX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MgVyJUPFb5xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FEATURE LAYER**"
      ],
      "metadata": {
        "id": "PEbMYTBAb-In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pilih feature column mana yang akan digunakan\n",
        "feature_columns = []\n",
        "# numeric column\n",
        "for header in ['carat', 'depth', 'x', 'y', 'z']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "#membuat feature layer\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "rcTT4pr1b5Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN MODEL**"
      ],
      "metadata": {
        "id": "aWIXgGVtcCoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dropout(.1),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=10)\n"
      ],
      "metadata": {
        "id": "CfW1O5QFcABx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}